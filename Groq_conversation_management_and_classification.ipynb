{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSseo8ckUyQY"
      },
      "outputs": [],
      "source": [
        "# Install & imports\n",
        "!pip install openai jsonschema --quiet\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "import openai\n",
        "import requests\n",
        "from jsonschema import validate, ValidationError\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# set Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "pI578OhUWOAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Api Test\n",
        "resp = client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"explain how Groq work simply\"}]\n",
        ")\n",
        "print(resp.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1BxnD6l6aUsS",
        "outputId": "4a0e438b-0c0c-4259-a197-14fc1fe3be6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq is a company specializing in chip and software solutions for accelerating machine learning (ML) workloads. I'll break down how they work simply:\n",
            "\n",
            "**Hardware Component: Groq Chip**\n",
            "\n",
            "Groq's AI chip is designed to improve the performance and efficiency of ML computations. The chip consists of multiple key components:\n",
            "\n",
            "1. **Highly Parallel Architecture**: It has thousands of processors and AI cores that can run many operations simultaneously, accelerating training and inference tasks.\n",
            "2. **High Bandwidth Memory**: The chip has access to high-speed memory that reduces data transfer bottlenecks, allowing for faster access to data.\n",
            "3. **Low Latency and High Throughput**: The chip is optimized for low latency and high throughput, making it suitable for real-time AI applications.\n",
            "\n",
            "**Software Component: Groq Platform**\n",
            "\n",
            "The Groq platform is a suite of software tools that work with the AI chip to accelerate ML workloads. It includes:\n",
            "\n",
            "1. **Compiler and Runtime**: This part optimizes code for execution on the Groq chip, removing the need for manual tuning and reducing development time.\n",
            "2. **TensorFlow and PyTorch Integration**: The platform integrates with popular ML frameworks like TensorFlow and PyTorch, allowing developers to use familiar APIs for building and deploying AI models.\n",
            "3. **Cloud and Edge Deployment**: The Groq platform provides tools for deploying AI models on-premises, in the cloud, or at the edge, allowing for flexibility and scalability.\n",
            "\n",
            "**Key Benefits**\n",
            "\n",
            "Using Groq's chip and platform combination offers several benefits:\n",
            "\n",
            "1. **Accelerated Training and Inference**: The highly parallel architecture and optimized software reduce training and inference times by several orders of magnitude.\n",
            "2. **Reduced Power Consumption**: The chip and platform are designed to minimize power consumption, making them suitable for use in edge devices and data centers.\n",
            "3. **Improved Performance**: The combination of high-bandwidth memory and low-latency processing enables faster AI model development and deployment.\n",
            "\n",
            "Overall, Groq's solution empowers developers and researchers to accelerate ML workloads, reduce development time, and improve performance, all while minimizing power consumption.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=GROQ_API_KEY,\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "GROQ_BASE = \"https://api.groq.com/openai/v1\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "RtvYPMp-VdR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# configure Groq client once\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, model=\"llama-3.1-8b-instant\", summarization_trigger_k=3):\n",
        "        self.model = model\n",
        "        self.history = []\n",
        "        self.summarization_trigger_k = summarization_trigger_k\n",
        "        self.append_count = 0\n",
        "\n",
        "    def append(self, role, content):\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        self.append_count += 1\n",
        "\n",
        "        # check if summarization should trigger\n",
        "        if self.append_count % self.summarization_trigger_k == 0:\n",
        "            summary = self.summarize()\n",
        "            self._replace_summary(summary)\n",
        "            return {\"summarized\": True, \"summary\": summary}\n",
        "        else:\n",
        "            return {\"summarized\": False}\n",
        "\n",
        "    def summarize(self):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Summarize this conversation concisely.\"},\n",
        "                    {\"role\": \"user\", \"content\": str(self.history)}\n",
        "                ],\n",
        "            )\n",
        "            # Correct extraction\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"(fallback summary due to API error: {e})\"\n",
        "\n",
        "    def _replace_summary(self, summary):\n",
        "        # remove any existing auto-summary\n",
        "        self.history = [msg for msg in self.history if not (\n",
        "            msg[\"role\"] == \"system\" and msg[\"content\"].startswith(\"Conversation summary (auto):\")\n",
        "        )]\n",
        "        # insert new one at the start\n",
        "        self.history.insert(0, {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"Conversation summary (auto):\\n{summary}\"\n",
        "        })\n",
        "\n",
        "    def truncate_last_n_turns(self, n):\n",
        "        # keep system + last n turns\n",
        "        system_msgs = [m for m in self.history if m[\"role\"] == \"system\"]\n",
        "        non_system_msgs = [m for m in self.history if m[\"role\"] != \"system\"]\n",
        "        self.history = system_msgs + non_system_msgs[-n:]\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history\n",
        "\n",
        "    def truncate_last_n_turns(self, n: int):\n",
        "        # each \"turn\" here is considered one message. If you want one user+assistant as a turn, adjust accordingly.\n",
        "        if n <= 0:\n",
        "            return\n",
        "        self.history = self.history[-n:]\n",
        "\n",
        "    def truncate_by_char_limit(self, char_limit: int):\n",
        "        # keep messages until char_limit is satisfied, prioritize recent messages\n",
        "        new_hist = []\n",
        "        total = 0\n",
        "        for msg in reversed(self.history):\n",
        "            length = len(msg[\"content\"])\n",
        "            if total + length > char_limit:\n",
        "                break\n",
        "            new_hist.append(msg)\n",
        "            total += length\n",
        "        self.history = list(reversed(new_hist))\n",
        "\n",
        "    def truncate_by_word_limit(self, word_limit: int):\n",
        "        new_hist = []\n",
        "        total = 0\n",
        "        for msg in reversed(self.history):\n",
        "            words = len(msg[\"content\"].split())\n",
        "            if total + words > word_limit:\n",
        "                break\n",
        "            new_hist.append(msg)\n",
        "            total += words\n",
        "        self.history = list(reversed(new_hist))\n",
        "\n",
        "    def summarize_history(self, prompt_prefix: Optional[str]=None, max_tokens:int=400) -> str:\n",
        "        \"\"\"\n",
        "        Summarize current history by sending to the model.\n",
        "        Returns plain text summary.\n",
        "        \"\"\"\n",
        "        if not self.history:\n",
        "            return \"\"\n",
        "\n",
        "        # Prepare summary prompt: include the concatenated conversation with roles\n",
        "        convo_text = \"\\n\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.history])\n",
        "        system_prompt = \"You are a concise summarizer. Produce a short summary capturing the key points, actions, and any named entities.\"\n",
        "\n",
        "        if prompt_prefix:\n",
        "            user_prompt = prompt_prefix + \"\\n\\nConversation:\\n\" + convo_text\n",
        "        else:\n",
        "            user_prompt = \"Summarize the following conversation briefly, focusing on key facts, requested actions, and any personal information:\\n\\n\" + convo_text\n",
        "\n",
        "        # Use Groq via OpenAI-compatible client\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=self.summary_model,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":system_prompt},\n",
        "                    {\"role\":\"user\",\"content\":user_prompt}\n",
        "                ],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            # Groq returns choices similar to OpenAI; adapt if necessary\n",
        "            # Attempt robust extraction:\n",
        "            text = None\n",
        "            if hasattr(resp, \"choices\"):\n",
        "                text = resp.choices[0].message[\"content\"]\n",
        "            elif isinstance(resp, dict) and \"choices\" in resp:\n",
        "                text = resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "            elif hasattr(resp, \"output\") and resp.output:\n",
        "                # some Groq endpoints return output\n",
        "                text = resp.output[0].get(\"content\", \"\")\n",
        "            else:\n",
        "                text = str(resp)\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            # Fallback to local simplistic summarizer (if no API key or call fails)\n",
        "            # naive: take first and last message condensed\n",
        "            fallback = \" | \".join([self.history[0][\"content\"][:200], self.history[-1][\"content\"][:200]])\n",
        "            return f\"(fallback summary due to API error: {e}) {fallback}\"\n"
      ],
      "metadata": {
        "id": "NIA1IR1FVdd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConversationManager(model=\"llama-3.1-8b-instant\", summarization_trigger_k=3)\n",
        "\n",
        "# feed some conversation samples\n",
        "samples = [\n",
        "    (\"user\", \"Hi, I want help with my project. It's an app to recommend fertilizers.\"),\n",
        "    (\"assistant\", \"Sure — what's the target platform and what data do you have?\"),\n",
        "    (\"user\", \"Mobile, soil data and weather. I also want a summarizer module.\"),\n",
        "    (\"assistant\", \"Good. You should structure conversations and summarize periodically.\"),\n",
        "    (\"user\", \"Make an endpoint for summarization; keep logs.\"),\n",
        "    (\"assistant\", \"I'll draft the endpoint and schema.\"),\n",
        "]\n",
        "\n",
        "outputs = []\n",
        "for r,c in samples:\n",
        "    out = cm.append(r, c)\n",
        "    outputs.append(out)\n",
        "\n",
        "print(\"After feeding 5 messages (summarization runs at every 3rd append):\")\n",
        "print(\"Run outputs (shows if summarization happened):\")\n",
        "print(outputs)\n",
        "\n",
        "print(\"\\nCurrent history:\")\n",
        "for m in cm.get_history():\n",
        "    print(m[\"role\"], \":\", m[\"content\"][:200])\n",
        "\n",
        "# Demonstrate truncation options\n",
        "cm.append(\"user\", \"Extra message to trigger summarization (3rd run).\")\n",
        "print(\"\\nAfter another append (should have triggered summarization at 6th run if k=3):\")\n",
        "for m in cm.get_history():\n",
        "    print(m[\"role\"], \":\", (m[\"content\"][:300] + '...') if len(m[\"content\"])>300 else m[\"content\"])\n",
        "\n",
        "# Truncate example: keep last 2 messages\n",
        "cm.truncate_last_n_turns(2)\n",
        "print(\"\\nAfter truncating to last 2 messages:\")\n",
        "for m in cm.get_history():\n",
        "    print(m)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fSN77gWQVdgy",
        "outputId": "79a4b307-6a19-4447-d42d-7589ca8c602f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After feeding 5 messages (summarization runs at every 3rd append):\n",
            "Run outputs (shows if summarization happened):\n",
            "[{'summarized': False}, {'summarized': False}, {'summarized': True, 'summary': \"You're developing an app that recommends fertilizers to users based on soil data and weather information, primarily for mobile platforms. You also want to include a summarizer module.\"}, {'summarized': False}, {'summarized': False}, {'summarized': True, 'summary': 'The conversation is about developing an app that recommends fertilizers based on soil data and weather information for mobile platforms. The key points include:\\n\\n- Developing an app for mobile platforms\\n- Using soil data and weather information for fertilizer recommendations\\n- Incorporating a summarizer module\\n- Creating an endpoint for summarization and keeping logs'}]\n",
            "\n",
            "Current history:\n",
            "system : Conversation summary (auto):\n",
            "The conversation is about developing an app that recommends fertilizers based on soil data and weather information for mobile platforms. The key points include:\n",
            "\n",
            "- Develop\n",
            "user : Hi, I want help with my project. It's an app to recommend fertilizers.\n",
            "assistant : Sure — what's the target platform and what data do you have?\n",
            "user : Mobile, soil data and weather. I also want a summarizer module.\n",
            "assistant : Good. You should structure conversations and summarize periodically.\n",
            "user : Make an endpoint for summarization; keep logs.\n",
            "assistant : I'll draft the endpoint and schema.\n",
            "\n",
            "After another append (should have triggered summarization at 6th run if k=3):\n",
            "system : Conversation summary (auto):\n",
            "The conversation is about developing an app that recommends fertilizers based on soil data and weather information for mobile platforms. The key points include:\n",
            "\n",
            "- Developing an app for mobile platforms\n",
            "- Using soil data and weather information for fertilizer recommendat...\n",
            "user : Hi, I want help with my project. It's an app to recommend fertilizers.\n",
            "assistant : Sure — what's the target platform and what data do you have?\n",
            "user : Mobile, soil data and weather. I also want a summarizer module.\n",
            "assistant : Good. You should structure conversations and summarize periodically.\n",
            "user : Make an endpoint for summarization; keep logs.\n",
            "assistant : I'll draft the endpoint and schema.\n",
            "user : Extra message to trigger summarization (3rd run).\n",
            "\n",
            "After truncating to last 2 messages:\n",
            "{'role': 'assistant', 'content': \"I'll draft the endpoint and schema.\"}\n",
            "{'role': 'user', 'content': 'Extra message to trigger summarization (3rd run).'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from jsonschema import validate, ValidationError\n",
        "from openai import OpenAI\n",
        "\n",
        "# Groq client setup\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "# JSON Schema\n",
        "person_schema = {\n",
        "    \"name\": \"person_info\",\n",
        "    \"description\": \"Extract name, email, phone, location and age if present. Return null if missing.\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": [\"string\", \"null\"]},\n",
        "        \"email\": {\"type\": [\"string\", \"null\"], \"format\": \"email\"},\n",
        "        \"phone\": {\"type\": [\"string\", \"null\"]},\n",
        "        \"location\": {\"type\": [\"string\", \"null\"]},\n",
        "        \"age\": {\"type\": [\"integer\", \"null\"], \"minimum\": 0, \"maximum\": 130}\n",
        "    },\n",
        "    \"required\": []\n",
        "}\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"extract_person_info\",\n",
        "        \"description\": \"Extract structured person info (name, email, phone, location, age) from a chat text.\",\n",
        "        \"parameters\": person_schema\n",
        "    }\n",
        "]\n",
        "\n",
        "# Sample Chats\n",
        "sample_chats = [\n",
        "    \"Hey, I'm Amit Deo. You can reach me at amit@example.com or on +91-9876543210. I'm 22 and based in Pune.\",\n",
        "    \"Hello, this is Priya. My email is priya.work@mail.com. I live in Bengaluru. Age: 24.\",\n",
        "    \"User: Hi. I'm interested, contact: 555-1234. No email yet. Name: Rahul, Location: Delhi.\"\n",
        "]\n",
        "\n",
        "# Function Caller\n",
        "def call_function_extract(chat_text):\n",
        "    \"\"\"\n",
        "    Call Groq API (OpenAI-compatible function calling) asking to 'call' extract_person_info\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a JSON extractor. Use the function with the exact schema to return valid JSON.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Extract info from this chat:\\n\\n{chat_text}\"}\n",
        "    ]\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"llama-3.1-8b-instant\",\n",
        "            messages=messages,\n",
        "            functions=functions,\n",
        "            function_call={\"name\": \"extract_person_info\"},  # force function call\n",
        "            max_tokens=400,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "\n",
        "        choice = resp.choices[0].message\n",
        "\n",
        "        if choice.function_call:  # model chose function call\n",
        "            args_text = choice.function_call.arguments\n",
        "            parsed = json.loads(args_text)\n",
        "            return parsed, resp\n",
        "\n",
        "        if choice.content:  # model may fallback to JSON content\n",
        "            try:\n",
        "                parsed = json.loads(choice.content)\n",
        "                return parsed, resp\n",
        "            except Exception:\n",
        "                return {\"error\": \"Could not parse JSON from model output\", \"raw\": choice.content}, resp\n",
        "\n",
        "        return {\"error\": \"No usable output\"}, resp\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}, None\n",
        "\n",
        "# ---------------- Demo ----------------\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    parsed, raw_resp = call_function_extract(chat)\n",
        "    print(f\"\\nSample {i} input:\\n{chat}\\nParsed output:\\n{parsed}\")\n",
        "    try:\n",
        "        validate(instance=parsed, schema=person_schema)\n",
        "        print(\"Validation: OK\")\n",
        "    except ValidationError as ve:\n",
        "        print(\"Validation ERROR:\", ve)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxUQrigDVdjm",
        "outputId": "de832de1-7fa4-4178-e530-6a665b915a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample 1 input:\n",
            "Hey, I'm Amit Deo. You can reach me at amit@example.com or on +91-9876543210. I'm 22 and based in Pune.\n",
            "Parsed output:\n",
            "{'age': 22, 'email': 'amit@example.com', 'location': 'Pune', 'name': 'Amit Deo', 'phone': '+91-9876543210'}\n",
            "Validation: OK\n",
            "\n",
            "Sample 2 input:\n",
            "Hello, this is Priya. My email is priya.work@mail.com. I live in Bengaluru. Age: 24.\n",
            "Parsed output:\n",
            "{'age': 24, 'email': 'priya.work@mail.com', 'location': 'Bengaluru', 'name': 'Priya', 'phone': None}\n",
            "Validation: OK\n",
            "\n",
            "Sample 3 input:\n",
            "User: Hi. I'm interested, contact: 555-1234. No email yet. Name: Rahul, Location: Delhi.\n",
            "Parsed output:\n",
            "{'age': None, 'email': None, 'location': 'Delhi', 'name': 'Rahul', 'phone': '555-1234'}\n",
            "Validation: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gaD1ovhVdpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}